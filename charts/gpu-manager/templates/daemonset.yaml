apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: gpu-manager
  labels:
    draft: {{ default "draft-app" .Values.draft }}
    chart: "{{ .Chart.Name }}-{{ .Chart.Version | replace "+" "_" }}"
spec:
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      # This annotation is deprecated. Kept here for backward compatibility
      # See https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ""
{{- if .Values.podAnnotations }}
{{ toYaml .Values.podAnnotations | indent 8 }}
{{- end }}
      labels:
        draft: {{ default "draft-app" .Values.draft }}
        app: gpu-manager
    spec:
      # only run node hash gpu device
      nodeSelector:
        nvidia-device-enable: enable
      tolerations:
      - key: nvidia.com/gpu
        operator: Equal
        value: present
        effect: NoSchedule
        # This toleration is deprecated. Kept here for backward compatibility
        # See https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/
      - key: CriticalAddonsOnly
        operator: Exists
      - key: tencent.com/vcuda-core
        operator: Exists
        effect: NoSchedule
      # Mark this pod as a critical add-on; when enabled, the critical add-on
      # scheduler reserves resources for critical add-on pods so that they can
      # be rescheduled after a failure.
      # See https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/
      priorityClassName: "system-node-critical"
      hostNetwork: true
      hostPID: true
      initContainers:
      - image: "cos-nvidia-installer:fixed"
        imagePullPolicy: {{ .Values.image.pullPolicy }}
        name: nvidia-driver-installer
        resources:
{{ toYaml .Values.resources | indent 12 }}
        securityContext:
          privileged: true
          runAsUser: 0
          runAsGroup: 0
        env:
        - name: NVIDIA_INSTALL_DIR_HOST
          value: /home/kubernetes/bin/nvidia
        - name: NVIDIA_INSTALL_DIR_CONTAINER
          value: /usr/local/nvidia
        - name: VULKAN_ICD_DIR_HOST
          value: /home/kubernetes/bin/nvidia/vulkan/icd.d
        - name: VULKAN_ICD_DIR_CONTAINER
          value: /etc/vulkan/icd.d
        - name: ROOT_MOUNT_DIR
          value: /root
        - name: COS_TOOLS_DIR_HOST
          value: /var/lib/cos-tools
        - name: COS_TOOLS_DIR_CONTAINER
          value: /build/cos-tools
        # use latest version
        # - name: NVIDIA_DRIVER_VERSION
        #   value: "440.64.00"
        volumeMounts:
        - name: nvidia-dir
          mountPath: /usr/local/nvidia
        - name: vulkan-icd-mount
          mountPath: /etc/vulkan/icd.d
        - name: dev-dir
          mountPath: /dev
        - name: root-mount
          mountPath: /root
        - name: cos-tools
          mountPath: /build/cos-tools
      containers:
      - image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
        imagePullPolicy: {{ .Values.image.pullPolicy }}
        name: gpu-manager
        securityContext:
          privileged: true
        ports:
        - containerPort: {{ .Values.service.internalPort }}
        command:
        - sh
        - -c
        - >
          ldconfig
          &&
          gpu-manager
          --extra-config=/home/kubernetes/bin/gpu-manager/config/extra-config.json
          --v=4
          --hostname-override=${NODE_NAME}
          --share-mode=true
          --volume-config=/home/kubernetes/bin/gpu-manager/config/volume.conf
          --log-dir=/var/log/gpu-manager
          --query-addr=0.0.0.0
          --incluster-mode=true
          --docker-endpoint=unix:////usr/local/run/docker.sock
          --virtual-manager-path=/home/kubernetes/bin/gpu-manager/vm
        livenessProbe:
          tcpSocket:
            port: {{ .Values.service.internalPort }}
          initialDelaySeconds: {{ .Values.livenessProbe.initialDelaySeconds }}
          periodSeconds: {{ .Values.livenessProbe.periodSeconds }}
          successThreshold: {{ .Values.livenessProbe.successThreshold }}
          timeoutSeconds: {{ .Values.livenessProbe.timeoutSeconds }}
        readinessProbe:
          tcpSocket:
            port: {{ .Values.service.internalPort }}
          periodSeconds: {{ .Values.readinessProbe.periodSeconds }}
          successThreshold: {{ .Values.readinessProbe.successThreshold }}
          timeoutSeconds: {{ .Values.readinessProbe.timeoutSeconds }}
        volumeMounts:
        - name: gpu-manager-config
          mountPath: /home/kubernetes/bin/gpu-manager/config
          readOnly: true
        - name: device-plugin
          mountPath: /var/lib/kubelet/device-plugins
        - name: vdriver
          mountPath: /home/kubernetes/bin/gpu-manager/vdriver
        - name: vmdata
          mountPath: /home/kubernetes/bin/gpu-manager/vm
        - name: log
          mountPath: /var/log/gpu-manager
        - name: run-dir
          # mountPath: /var/run
          # mounting it in /var/run would conflict with /var/run/secrets/kubernetes.io/serviceaccount
          mountPath: /usr/local/run
          readOnly: true
        - name: cgroup
          mountPath: /sys/fs/cgroup
          readOnly: true
        - name: nvidia-dir
          mountPath: /usr/local/nvidia
          readOnly: true
        env:
{{- range $pkey, $pval := .Values.env }}
        - name: {{ $pkey }}
          value: {{ quote $pval }}
{{- end }}
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        envFrom:
{{ toYaml .Values.envFrom | indent 10 }}
        resources:
{{ toYaml .Values.resources | indent 12 }}
      terminationGracePeriodSeconds: {{ .Values.terminationGracePeriodSeconds }}
      serviceAccountName: gpu-manager
      volumes:
      - name: gpu-manager-config
        configMap:
          name: gpu-manager-config
      - name: device-plugin
        hostPath:
          type: Directory
          path: /var/lib/kubelet/device-plugins
      - name: vmdata
        hostPath:
          type: DirectoryOrCreate
          path: /home/kubernetes/bin/gpu-manager/vm
      - name: vdriver
        hostPath:
          type: DirectoryOrCreate
          path: /home/kubernetes/bin/gpu-manager/vdriver
      - name: log
        hostPath:
          type: DirectoryOrCreate
          path: /var/log/gpu-manager
        # We have to mount the whole /var/run directory into container, because of bind mount docker.sock
        # inode change after host docker is restarted
      - name: run-dir
        hostPath:
          type: Directory
          path: /var/run
      - name: cgroup
        hostPath:
          type: Directory
          path: /sys/fs/cgroup
        # We have to mount /usr directory instead of specified library path, because of non-existing
        # problem for different distro
      - name: nvidia-dir
        hostPath:
          type: Directory
          path: /home/kubernetes/bin/nvidia/
      - name: dev-dir
        hostPath:
          path: /dev
      - name: root-mount
        hostPath:
          path: /
      - name: cos-tools
        hostPath:
          # path: /var/lib/cos-tools # not executable
          path: /home/kubernetes/bin/cos-tools
