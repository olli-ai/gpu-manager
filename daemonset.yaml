apiVersion: v1
kind: ConfigMap
metadata:
  name: gpu-manager-config
data:
  volume.conf: |-
    {
      "volume": [
        {
          "name": "nvidia",
          "base": "/home/kubernetes/bin/gpu-manager/vdriver",
          "mode": "ro",
          "components": {
            "binaries": [
              "gpu-client",
              "nvidia-cuda-mps-control",
              "nvidia-cuda-mps-server",
              "nvidia-debugdump",
              "nvidia-persistenced",
              "nvidia-smi"
            ],
            "libraries": [
              "libcuda-control.so",
              "libnvidia-ml.so",
              "libcuda.so",
              "libnvidia-ptxjitcompiler.so",
              "libnvidia-fatbinaryloader.so",
              "libnvidia-opencl.so",
              "libnvidia-compiler.so",
              "libvdpau_nvidia.so",
              "libnvidia-encode.so",
              "libnvcuvid.so",
              "libnvidia-fbc.so",
              "libnvidia-ifr.so",
              "libGL.so",
              "libGLX.so",
              "libOpenGL.so",
              "libGLESv1_CM.so",
              "libGLESv2.so",
              "libEGL.so",
              "libGLdispatch.so",
              "libGLX_nvidia.so",
              "libEGL_nvidia.so",
              "libGLESv2_nvidia.so",
              "libGLESv1_CM_nvidia.so",
              "libnvidia-eglcore.so",
              "libnvidia-egl-wayland.so",
              "libnvidia-glcore.so",
              "libnvidia-tls.so",
              "libnvidia-glsi.so",
              "libnvidia-opticalflow.so"
            ]
          }
        },
        {
          "name": "origin",
          "base": "/home/kubernetes/bin/gpu-manager/vdriver",
          "mode": "ro",
          "components": {
            "binaries": [
              "nvidia-cuda-mps-control",
              "nvidia-cuda-mps-server",
              "nvidia-debugdump",
              "nvidia-persistenced",
              "nvidia-smi"
            ],
            "libraries": [
              "libnvidia-ml.so",
              "libcuda.so",
              "libnvidia-ptxjitcompiler.so",
              "libnvidia-fatbinaryloader.so",
              "libnvidia-opencl.so",
              "libnvidia-compiler.so",
              "libvdpau_nvidia.so",
              "libnvidia-encode.so",
              "libnvcuvid.so",
              "libnvidia-fbc.so",
              "libnvidia-ifr.so",
              "libGL.so",
              "libGLX.so",
              "libOpenGL.so",
              "libGLESv1_CM.so",
              "libGLESv2.so",
              "libEGL.so",
              "libGLdispatch.so",
              "libGLX_nvidia.so",
              "libEGL_nvidia.so",
              "libGLESv2_nvidia.so",
              "libGLESv1_CM_nvidia.so",
              "libnvidia-eglcore.so",
              "libnvidia-egl-wayland.so",
              "libnvidia-glcore.so",
              "libnvidia-tls.so",
              "libnvidia-glsi.so",
              "libnvidia-opticalflow.so"
            ]
          }
        }
      ]
    }

  extra-config.json: "{}"
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: gpu-manager
  labels:
    app: gpu-manager
spec:
  selector:
    matchLabels:
      app: gpu-manager
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      # This annotation is deprecated. Kept here for backward compatibility
      # See https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ""
      labels:
        app: gpu-manager
    spec:
      # only run node hash gpu device
      nodeSelector:
        cloud.google.com/gke-nodepool: gpu-1
      tolerations:
      - key: nvidia.com/gpu
        operator: Equal
        value: present
        effect: NoSchedule
        # This toleration is deprecated. Kept here for backward compatibility
        # See https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/
      - key: CriticalAddonsOnly
        operator: Exists
      - key: tencent.com/vcuda-core
        operator: Exists
        effect: NoSchedule
      # Mark this pod as a critical add-on; when enabled, the critical add-on
      # scheduler reserves resources for critical add-on pods so that they can
      # be rescheduled after a failure.
      # See https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/
      priorityClassName: "system-node-critical"
      hostNetwork: true
      hostPID: true
      initContainers:
      - image: "cos-nvidia-installer:fixed"
        imagePullPolicy: IfNotPresent
        name: nvidia-driver-installer
        securityContext:
          privileged: true
          runAsUser: 0
          runAsGroup: 0
        env:
        - name: NVIDIA_INSTALL_DIR_HOST
          value: /home/kubernetes/bin/nvidia
        - name: NVIDIA_INSTALL_DIR_CONTAINER
          value: /usr/local/nvidia
        - name: VULKAN_ICD_DIR_HOST
          value: /home/kubernetes/bin/nvidia/vulkan/icd.d
        - name: VULKAN_ICD_DIR_CONTAINER
          value: /etc/vulkan/icd.d
        - name: ROOT_MOUNT_DIR
          value: /root
        - name: COS_TOOLS_DIR_HOST
          value: /var/lib/cos-tools
        - name: COS_TOOLS_DIR_CONTAINER
          value: /build/cos-tools
        # use latest version
        # - name: NVIDIA_DRIVER_VERSION
        #   value: "440.64.00"
        volumeMounts:
        - name: nvidia-dir
          mountPath: /usr/local/nvidia
        - name: vulkan-icd-mount
          mountPath: /etc/vulkan/icd.d
        - name: dev-dir
          mountPath: /dev
        - name: root-mount
          mountPath: /root
        - name: cos-tools
          mountPath: /build/cos-tools
      containers:
      - image: asia.gcr.io/olli-iviet/gpu-manager
        imagePullPolicy: IfNotPresent
        name: gpu-manager
        securityContext:
          privileged: true
        ports:
        - containerPort: 5678
        command:
        - sh
        - -c
        - >
          ldconfig
          &&
          gpu-manager
          --extra-config=/home/kubernetes/bin/gpu-manager/config/extra-config.json
          --v=3
          --hostname-override=${NODE_NAME}
          --share-mode=true
          --volume-config=/home/kubernetes/bin/gpu-manager/config/volume.conf
          --log-dir=/var/log/gpu-manager
          --query-addr=0.0.0.0
          --incluster-mode=true
          --docker-endpoint=unix:////usr/local/run/docker.sock
          --virtual-manager-path=/home/kubernetes/bin/gpu-manager/vm
        env:
        - name: LOGGER_LEVEL
          value: "8"
        livenessProbe:
          tcpSocket:
            port: 5678
        readinessProbe:
          tcpSocket:
            port: 5678
        volumeMounts:
        - name: gpu-manager-config
          mountPath: /home/kubernetes/bin/gpu-manager/config
          readOnly: true
        - name: device-plugin
          mountPath: /var/lib/kubelet/device-plugins
        - name: vdriver
          mountPath: /home/kubernetes/bin/gpu-manager/vdriver
        - name: vmdata
          mountPath: /home/kubernetes/bin/gpu-manager/vm
        - name: log
          mountPath: /var/log/gpu-manager
        - name: run-dir
          # mountPath: /var/run
          # mounting it in /var/run would conflict with /var/run/secrets/kubernetes.io/serviceaccount
          mountPath: /usr/local/run
          readOnly: true
        - name: cgroup
          mountPath: /sys/fs/cgroup
          readOnly: true
        - name: nvidia-dir
          mountPath: /usr/local/nvidia
          readOnly: true
        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
      serviceAccountName: gpu-manager
      volumes:
      - name: gpu-manager-config
        configMap:
          name: gpu-manager-config
      - name: device-plugin
        hostPath:
          type: Directory
          path: /var/lib/kubelet/device-plugins
      - name: vmdata
        hostPath:
          type: DirectoryOrCreate
          path: /home/kubernetes/bin/gpu-manager/vm
      - name: vdriver
        hostPath:
          type: DirectoryOrCreate
          path: /home/kubernetes/bin/gpu-manager/vdriver
      - name: log
        hostPath:
          type: DirectoryOrCreate
          path: /var/log/gpu-manager
        # We have to mount the whole /var/run directory into container, because of bind mount docker.sock
        # inode change after host docker is restarted
      - name: run-dir
        hostPath:
          type: Directory
          path: /var/run
      - name: cgroup
        hostPath:
          type: Directory
          path: /sys/fs/cgroup
        # We have to mount /usr directory instead of specified library path, because of non-existing
        # problem for different distro
      - name: nvidia-dir
        hostPath:
          type: Directory
          path: /home/kubernetes/bin/nvidia/
      - name: dev-dir
        hostPath:
          path: /dev
      - name: root-mount
        hostPath:
          path: /
      - name: cos-tools
        hostPath:
          # path: /var/lib/cos-tools # not executable
          path: /home/kubernetes/bin/cos-tools
      - name: vulkan-icd-mount
        hostPath:
          path: /home/kubernetes/bin/nvidia/vulkan/icd.d
